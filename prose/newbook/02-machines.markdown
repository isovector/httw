## Machines

All of computation is based upon extremely precise logical pieces of reasoning.
Mind-boggling amounts of tiny, predictable machines are laid out in complicated
patterns. These machines push a small amount of electricity under some
circumstances, and not under others. At every layer of the system, the entire
process of moving electrons is deterministic and prescribed. There is no
absolutely no intelligence, intuition, or randomness whatsoever in the system,
and yet it still behaves in ways that are meaningful and surprising to humans.
How can this be? How can meaning arise from millions of tiny, mindless machines
shuffling electrons around based on predetermined rules?

The answer to this question is the core of the book you're currently reading,
and it comes in many different flavors. In one sense, this is a question of
engineering --- what procedure can be followed to build a system with these
properties? What are the governing dynamics underlying this natural phenomenon?

But in another sense, this question is highly philosophical. Where does meaning
come from, and why should we expect any relationship between the world of
electron flow and human understanding? Why should these symbolic systems map to
reality?

Why do these systems map to reality? The answer is rather tongue-in-cheek, but
entirely true: *because we can't build anything else.* There are astronomically
large numbers of possible machines that could exist. More than every star in the
sky; more than every *atom in the universe.* There are so many possible machines
that there are no words that give any sense of understanding of the sheer
number. We must rely on analogy.

Imagine if you were tasked with finding one particular grain of sand on Earth.
There are (very) approximately 7,500,000,000,000,000,000 grains of sand on the
planet, so even if you could inspect one grain every second, and you did nothing
else --- not even sleeping or eating --- for your entire life of 100 years,
you'd still need, on average, 2.5 *billion* lifetimes to find the one you're
looking for.

While that sounds hard, it's still nowhere near the difficulty of finding a
particular machine in the space of all possible computations. Let me explain.

Two and a half billion lifetimes to find a grain of sand doesn't sound like it
will be happening anytime soon, but it's still a tractable problem. Seeing as
there's about seven billion people, if you could enlist them all, you'd have
more than the requisite number of lifetimes to dedicate to the problem.

This is what's known as a *linear* system. If you throw twice as many resources
at the problem, you get the result in half as much time. Most systems that
humans encounter in their day-to-day life are linear. The big outliers are
population growth and investments --- your population grows faster the more
people you have, which begets faster growth again, ad infinitum. Interest from
investments is similar, as you gain interest, you have more money, of which you
can now gain more interest off of. Systems that grow like this are said to grow
*exponentially.*

Computer scientists are very interested in how quickly their systems (and
problems) grow --- so much so that they've invented an entire taxonomy of
classification for growth rates.

Let's return to our problem of finding any particular grain of sand. Even as a
linear problem it's already quite hard, and would require the coordination of
the entire human race in order to reliably solve. But imagine now if the number
of grains of sand weren't just 7.5 quintillion. Imagine if the grains of sand
were doubling *every second.* The problem that before required every human to
give up their entire life to help solve has now doubled in difficulty 3 million
times. That's not to say it's 3 million times harder, but that it's become twice
as hard, and then twice as hard again, and then twice as hard *again...* three
million times.

At this scale, it's inconceivable to think that humans can ever tackle this
problem. It's not an issue of speed, but of *growth.* Even if humans colonized
every planet in the solar system, and each one could check a billion grains of
sand every second, we'd still never get close to having a chance.

As computations grow in size (that is to say, in *potential interest* to
humans,) the number of programs that could possibly exist grow roughly as
quickly as our hypothetical dividing grains of sand. But unlike sand, we can't
just cast our gaze on the beach in order to locate arbitrary grains. The only
way to get your hands on a complicated machine is to get your hands on a
slightly less complicated machine and make it a litter bigger.

This argument starts to suggest why human-relevant computation is possible ---
because computers need to be made by humans, and the only way we can make them
is by slowly extending machines we already know and understand. That is to say,
the human mind is *literally blind* to the existence of the crushing majority of
possibilities. In a sense, most programs are so meaningless that they're
impossible for humans to think about, and thus, the only ones we *can* think
about are the ones relevant to us. That's not to say that these things don't
exist, only that we can never experience them.

It's not that we *don't* want meaningless machines. It's that we *can't* want
them.

Rather humbling and mystical, isn't it? There are fundamentally unknowable
things in the universe. Not because of non-existence, but because, in a sense,
of *too much* existence. It's a lot like being a fish in the ocean; the water
around you is so ubiquitous that it's not even noticeable. We're all just
swimming in computation, oblivious to all but those that are so special that
they stand out from the rest by being *meaningful.*

Returning from the philosophical detour to the question of how can we engineer
these sorts of systems, we find that we've already got an answer. Big,
comprehensible systems are made by combining smaller comprehensible systems. To
engineer a meaningful machine, we must start from the exceptionally simple ---
where there are so few in number that we *can* differentiate between them ---
and then intentionally build up from there.

The remainder of this chapter is dedicated to looking at these fundamental
building blocks. Literally everything else that can be analytically studied by
humans is constructed by combining these parts in different and interesting
patterns.

